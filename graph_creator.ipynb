{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import json\n",
    "import random\n",
    "import artm\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from pyclustering.cluster.xmeans import xmeans\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "from pyclustering.cluster.optics import optics\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bert_serving.client import BertClient\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_dataset_json = 'babi-1-6-full.json'\n",
    "vw_data_path = 'data/babi-st.vw'\n",
    "bigartm_batches_path = 'data/bigartm_batches_babi'\n",
    "max_st = 25\n",
    "num_topics = 2\n",
    "gephi_csv_path = 'data/babi-{}.csv'.format(max_st)\n",
    "!mkdir data/bigartm_batches_babi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/convai2_>3_batches_babi’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "dialog_dataset_json = 'convai2_>3.json'\n",
    "vw_data_path = 'data/convai2_>3-st.vw'\n",
    "bigartm_batches_path = 'data/convai2_>3_batches_babi'\n",
    "max_st = 25\n",
    "num_topics = 3\n",
    "gephi_csv_path = 'data/convai2_>3-{}.csv'.format(max_st)\n",
    "!mkdir 'data/convai2_>3_batches_babi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/convai1_batches_babi’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "dialog_dataset_json = 'f1-labeled-dialogs-with-dm.json'\n",
    "vw_data_path = 'data/convai1-st.vw'\n",
    "bigartm_batches_path = 'data/convai1_batches_babi'\n",
    "max_st = 30\n",
    "num_topics = 3\n",
    "gephi_csv_path = 'data/convai1-{}.csv'.format(max_st)\n",
    "!mkdir 'data/convai1_batches_babi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/multi-woz2_batches’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "dialog_dataset_json = 'data/multi-woz2.json'\n",
    "vw_data_path = 'data/multi-woz2-st.vw'\n",
    "bigartm_batches_path = 'data/multi-woz2_batches'\n",
    "max_st = 13\n",
    "num_topics = 7\n",
    "gephi_csv_path = 'data/multi-woz2-{}.csv'.format(max_st)\n",
    "!mkdir 'data/multi-woz2_batches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "def convert_to_vw(text, id_):\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokens = [t.lower() for t in tokenizer.tokenize(text)]\n",
    "    processed = []\n",
    "    for t in tokens:\n",
    "        l = lmtzr.lemmatize(t)\n",
    "        if l not in stopwords_en:\n",
    "            processed.append(l)\n",
    "    counted = Counter(processed)\n",
    "    res_str = str(id_)\n",
    "    for k, v in counted.items():\n",
    "        if v != 1:\n",
    "            res_str = res_str + \" {}:{}\".format(k, v)\n",
    "        else:\n",
    "            res_str = res_str + \" {}\".format(k)\n",
    "    return res_str\n",
    "\n",
    "\n",
    "def convert_to_vw_data(sentences, vw_filename):\n",
    "    vw_file = open(vw_filename, 'w')\n",
    "    vw_data = []\n",
    "    ind = 0\n",
    "    for sent in sentences:\n",
    "        converted = convert_to_vw(sent, ind)\n",
    "        if len(converted.split(\" \")) > 1:\n",
    "            vw_data.append(convert_to_vw(sent, ind))\n",
    "            ind += 1\n",
    "    for row in vw_data:\n",
    "        print(row, file=vw_file)\n",
    "    vw_file.close()\n",
    "\n",
    "    \n",
    "def save_vw_to_file(sentences, vw_filename):\n",
    "    vw_file = open(vw_filename, 'w')\n",
    "    vw_data = []\n",
    "    for sent in sentences:\n",
    "        if len(sent.split(\" \")) > 1:\n",
    "            vw_data.append(sent)\n",
    "        else:\n",
    "            vw_data.append(\"this is noise entry for topic modelling\")\n",
    "    for row in vw_data:\n",
    "        print(row, file=vw_file)\n",
    "    vw_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_sentence_embedder():\n",
    "    # Other embedder https://tfhub.dev/google/universal-sentence-encoder-large/3\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "    embedder = hub.Module(module_url)\n",
    "    return embedder\n",
    "        \n",
    "def _embed_sentences(sentences):\n",
    "    embedder = _init_sentence_embedder()\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        embeddings = session.run(embedder(sentences))\n",
    "    return embeddings\n",
    "\n",
    "def _init_tfidf_vectorizer():\n",
    "    stop_words = []\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', norm='l2')\n",
    "    return vectorizer\n",
    "\n",
    "def _embed_sentences_with_tfidf(sentences):\n",
    "    vectorizer = _init_tfidf_vectorizer()\n",
    "    word_doc_matrix = vectorizer.fit_transform(sentences)\n",
    "    return {'vectorizer': vectorizer, 'word_doc_matrix': word_doc_matrix}\n",
    "\n",
    "def _embed_sentences_with_bert(sentences):\n",
    "    bc = BertClient()\n",
    "    return bc.encode(sentences)\n",
    "\n",
    "def _extract_df_data_naive(dialogs):\n",
    "    df_data = defaultdict(list)\n",
    "    id_ = 0\n",
    "    for dialog_id, dialog in dialogs.items():\n",
    "        current_ind_both = 1\n",
    "\n",
    "        thread = dialog['thread']\n",
    "\n",
    "        for row in thread:\n",
    "            text = row['text']\n",
    "            df_data['st'].append(current_ind_both)\n",
    "            df_data['sent'].append(text)\n",
    "            df_data['cluster_id'].append(None)\n",
    "            df_data['cluster_name'].append(None)\n",
    "            df_data['user_id'].append(row['userId'])\n",
    "            df_data['vw_sent'].append(convert_to_vw(text, id_))\n",
    "            df_data['topic_name'].append('')\n",
    "            df_data['topic_score'].append(0)\n",
    "            id_ += 1\n",
    "            current_ind_both += 1\n",
    "    return pd.DataFrame(data=df_data) \n",
    "\n",
    "\n",
    "def add_cluster_name_to_dialogs(json_filepath, df):\n",
    "    with open(json_filepath, 'r') as f:\n",
    "        dialogs = json.load(f)\n",
    "        \n",
    "    for dialog_id, dialog in dialogs.items():\n",
    "        thread = dialog['thread']\n",
    "\n",
    "        current_ind = 1\n",
    "\n",
    "        for row in thread:\n",
    "            text = row['text']\n",
    "            row['cluster_name'] = df[(df['st'] == current_ind) & (df['sent'] == text)].iloc[0]['topic_name_uniq_with_st']\n",
    "            current_ind += 1\n",
    "    return dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dialog_dataset_json, 'r') as f:\n",
    "    dialogs = json.load(f)\n",
    "dialogs = dict(list(dialogs.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = _extract_df_data_naive(dialogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    36680.000000\n",
       "mean        19.172083\n",
       "std         11.144493\n",
       "min          1.000000\n",
       "25%         10.000000\n",
       "50%         19.000000\n",
       "75%         28.000000\n",
       "max         54.000000\n",
       "Name: st, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.st.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0 0.0 2.3037147521972656\n",
      "topic_0: hello, hi, morning, good, morning\n",
      "topic_1: good, hi, hello\n",
      "------\n",
      "2 0.0 0.0 2.999999761581421\n",
      "topic_0: today, hello, help, help, hello\n",
      "topic_1: today\n",
      "------\n",
      "3 0.0 0.0 18.630420684814453\n",
      "topic_0: price, range, restaurant, make, reservation\n",
      "topic_1: table, book, may, like, food\n",
      "------\n",
      "4 0.0 0.0 6.0\n",
      "topic_0: topic, entry, noise, for, is\n",
      "topic_1: modelling, is, for, noise, entry\n",
      "------\n",
      "5 0.0 0.0 1.0\n",
      "topic_0: silence, silence\n",
      "topic_1: \n",
      "------\n",
      "6 0.0 0.0 6.822171688079834\n",
      "topic_0: modelling, entry, for, noise, topic\n",
      "topic_1: preference, cuisine, type, many, party\n",
      "------\n",
      "7 0.0 0.0 10.169947624206543\n",
      "topic_0: please, london, paris, people, six\n",
      "topic_1: food, love, cuisine, spanish, italian\n",
      "------\n",
      "8 0.0 0.0 9.48558235168457\n",
      "topic_0: look, ok, let, option, topic\n",
      "topic_1: party, would, many, people, range\n",
      "------\n",
      "9 0.0 0.0 8.320347785949707\n",
      "topic_0: please, people, two, price, range\n",
      "topic_1: silence, rome, bombay, madrid, please\n",
      "------\n",
      "10 0.0 0.0 11.28253173828125\n",
      "topic_0: look, option, let, ok, price\n",
      "topic_1: api_call, cheap, moderate, expensive, two\n",
      "------\n",
      "11 0.0 0.0 13.113056182861328\n",
      "topic_0: price, range, prefer, would, actually\n",
      "topic_1: silence, could, instead, people, six\n",
      "------\n",
      "12 0.0 0.0 11.256146430969238\n",
      "topic_0: let, ok, look, option, else\n",
      "topic_1: api_call, update, else, sure, anything\n",
      "------\n",
      "13 0.0 0.0 15.157787322998047\n",
      "topic_0: actually, prefer, would, price, range\n",
      "topic_1: silence, could, instead, people, food\n",
      "------\n",
      "14 0.0 0.0 8.37778377532959\n",
      "topic_0: api_call, cheap, moderate, ok, option\n",
      "topic_1: anything, sure, else, update, look\n",
      "------\n",
      "15 0.0 0.0 15.46724796295166\n",
      "topic_0: would, actually, prefer, noise, is\n",
      "topic_1: could, instead, silence, cuisine, people\n",
      "------\n",
      "16 0.0 0.0 6.034621238708496\n",
      "topic_0: let, look, ok, option, api_call\n",
      "topic_1: sure, else, anything, update, let\n",
      "------\n",
      "17 0.0 0.0 9.69354248046875\n",
      "topic_0: is, entry, modelling, for, noise\n",
      "topic_1: silence, could, instead, would, actually\n",
      "------\n",
      "18 0.0 0.0 11.051249504089355\n",
      "topic_0: option, look, ok, let, think\n",
      "topic_1: anything, else, sure, update, api_call\n",
      "------\n",
      "19 0.0 0.0 8.841836929321289\n",
      "topic_0: for, noise, is, topic, entry\n",
      "topic_1: silence, instead, could, doe, work\n",
      "------\n",
      "20 0.0 0.0 15.516172409057617\n",
      "topic_0: option, let, look, ok, think\n",
      "topic_1: api_call, moderate, two, sure, else\n",
      "------\n",
      "21 0.0 0.0 5.690927982330322\n",
      "topic_0: noise, for, entry, topic, is\n",
      "topic_1: silence, work, doe, like, look\n",
      "------\n",
      "22 0.0 0.0 17.101741790771484\n",
      "topic_0: option, think, let, find, sure\n",
      "topic_1: api_call, moderate, six, cheap, expensive\n",
      "------\n",
      "23 0.0 0.0 5.209566116333008\n",
      "topic_0: work, doe, something, else, phone\n",
      "topic_1: silence, like, address, great, look\n",
      "------\n",
      "24 0.0 0.0 17.377349853515625\n",
      "topic_0: option, think, let, find, sure\n",
      "topic_1: api_call, moderate, two, cheap, expensive\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, max_st):    \n",
    "    sentences = df[df.st == i]['vw_sent']\n",
    "    save_vw_to_file(sentences, vw_data_path)\n",
    "    batch_vectorizer = artm.BatchVectorizer(data_path=vw_data_path, data_format='vowpal_wabbit',\n",
    "                                        target_folder='{}/{}'.format(bigartm_batches_path, i))\n",
    "    lda = artm.LDA(num_topics=num_topics, alpha=0.01, beta=0.001,\n",
    "               num_document_passes=5, dictionary=batch_vectorizer.dictionary)\n",
    "    lda.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=100)\n",
    "    print(i, lda.sparsity_phi_last_value, lda.sparsity_theta_last_value, lda.perplexity_value[-1])\n",
    "    top_tokens = lda.get_top_tokens(num_tokens=5)\n",
    "    topics = {}    \n",
    "    for j, token_list in enumerate(top_tokens):\n",
    "        topic_name = 'topic_' + str(j)\n",
    "        topic_value = \", \".join(token_list)\n",
    "        print('{}: {}'.format(topic_name, topic_value))\n",
    "        topics[topic_name] = topic_value\n",
    "    print(\"------\")\n",
    "    \n",
    "    theta = lda.transform(batch_vectorizer)\n",
    "    sentences_topics = []\n",
    "    sentences_topics_scores = [] \n",
    "    for k in range(min(len(sentences), theta.shape[1])):\n",
    "        topics_distribution = theta[k]\n",
    "        top1_topic = sorted(list(topics_distribution.items()), key=lambda x: x[1], reverse=True)[0]\n",
    "        topic_name = top1_topic[0]        \n",
    "        topic_str = \", \".join(sorted(set(topics[topic_name].split(\", \"))))        \n",
    "        sentences_topics.append(topic_str)\n",
    "        sentences_topics_scores.append(top1_topic[1])\n",
    "    \n",
    "    df.loc[df.st == i, 'topic_name'] = sentences_topics\n",
    "    df.loc[df.st == i, 'topic_score'] = sentences_topics_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_topic_names_mapping = {}\n",
    "topic_names = list(set(df['topic_name']))\n",
    "for i in range(len(topic_names)):\n",
    "    t1 = set(topic_names[i].split(\", \"))\n",
    "    t1_str = \", \".join(sorted(t1))\n",
    "    for j in range(i+1, len(topic_names)):\n",
    "        t2 = set(topic_names[j].split(\", \"))\n",
    "        if len(t1 - t2) <= 1:\n",
    "            t2_str = \", \".join(sorted(t2))\n",
    "            similar_topic_names_mapping[t1_str] = t2_str    \n",
    "    if not similar_topic_names_mapping.get(t1_str):\n",
    "        similar_topic_names_mapping[t1_str] = t1_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_name_uniq'] = df['topic_name'].map(lambda x: similar_topic_names_mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name_to_st = defaultdict(set)\n",
    "for _, row in df[['topic_name_uniq', 'st']].iterrows():\n",
    "    topic_name_to_st[row['topic_name_uniq']].add(row['st'])\n",
    "\n",
    "orig_topic_names_to_names_with_st = {}\n",
    "for topic_name in topic_name_to_st.keys():    \n",
    "    speech_turns = [str(e) for e in sorted(topic_name_to_st[topic_name])]\n",
    "    orig_topic_names_to_names_with_st[topic_name] = \"{} [{}]\".format(topic_name, \",\".join(speech_turns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_name_uniq_with_st'] = df['topic_name_uniq'].map(lambda x: orig_topic_names_to_names_with_st[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_sent_map = {}\n",
    "for _, row in df.iterrows():\n",
    "    st_sent_map[(row['st'], row['sent'])] = row['topic_name_uniq_with_st']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st_sent_map = {}\n",
    "# for _, row in df.iterrows():\n",
    "#     st_sent_map[(row['st'], row['sent'])] = \"{} [{}]\".format(row['topic_name_uniq'], row['st'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff6ad0d8cd44d35b0ab7b86b8869d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(dialog_dataset_json, 'r') as f:\n",
    "    dialogs = json.load(f)\n",
    "        \n",
    "    for dialog_id, dialog in tqdm_notebook(dialogs.items()):\n",
    "        thread = dialog['thread']\n",
    "\n",
    "        current_ind = 1\n",
    "\n",
    "        for row in thread:\n",
    "            text = row['text']\n",
    "            row['cluster_name'] = st_sent_map[(current_ind, text)]\n",
    "            current_ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(save_filename, dialogs, step_max=15):\n",
    "    graph_dict = defaultdict(int)\n",
    "    for dialog_id, dialog in dialogs.items():\n",
    "        thread = dialog['thread']\n",
    "\n",
    "        current_ind = 1\n",
    "        prev_row = None\n",
    "        row = None\n",
    "        for ind in range(len(thread)):\n",
    "            if prev_row:\n",
    "                row = thread[ind]\n",
    "\n",
    "            if not prev_row:\n",
    "                prev_row = thread[ind]\n",
    "                row = None\n",
    "\n",
    "            if row and current_ind < step_max:\n",
    "                edge = (prev_row['cluster_name'], row['cluster_name'])\n",
    "                graph_dict[edge] += 1\n",
    "                current_ind += 1\n",
    "                prev_row = row\n",
    "                row = None\n",
    "    G = nx.DiGraph()\n",
    "    weighted_edges = [(k[0], k[1], v) for k, v in graph_dict.items()]\n",
    "    G.add_weighted_edges_from(weighted_edges)\n",
    "\n",
    "    m = nx.adjacency_matrix(G).todense().astype(float)\n",
    "    m = np.squeeze(np.asarray(m))\n",
    "\n",
    "    for arr in m.tolist():\n",
    "        str_arr = [str(e) for e in arr]\n",
    "        print(\",\".join(str_arr))\n",
    "    print(\"\\n\".join(list(G.nodes)))\n",
    "    with open(save_filename, 'w') as f:\n",
    "        nodes = list(G.nodes)\n",
    "        print(\";\" + \";\".join(nodes), file=f)\n",
    "        for ind, arr in enumerate(m.tolist()):\n",
    "            str_arr = [nodes[ind]]\n",
    "            str_arr += [str(e) for e in arr]\n",
    "            print(\";\".join(str_arr), file=f)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0,1000.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,517.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,483.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,517.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,1281.0,0.0,0.0,596.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,60.0,194.0,0.0,0.0,0.0,0.0,81.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,362.0,0.0,103.0,0.0,322.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,638.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,267.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,512.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,37.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,128.0,0.0,0.0,282.0,0.0,246.0,665.0,0.0,0.0,0.0,338.0,392.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,451.0,482.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,286.0,0.0,0.0,108.0,0.0,556.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,137.0,0.0,0.0,315.0,0.0,960.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,234.0,0.0,189.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,230.0,0.0,0.0,0.0,0.0,408.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,616.0,0.0,0.0,0.0,0.0,468.0,345.0,0.0,449.0,0.0,338.0,276.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,108.0,0.0,0.0,0.0,0.0,568.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,298.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,370.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,106.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,324.0,0.0\n",
      "0.0,0.0,0.0,0.0,855.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,440.0,0.0,0.0,0.0,0.0,0.0,0.0,312.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,563.0,0.0,116.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,145.0,0.0,0.0,0.0,236.0,168.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,239.0,0.0,0.0,0.0,0.0,0.0,0.0,9.0\n",
      "0.0,0.0,0.0,483.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,144.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,494.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,164.0,0.0,411.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,84.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,367.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,558.0,0.0,248.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,292.0,0.0,29.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "good, hello, hi [1]\n",
      "hello, help, today [2]\n",
      "book, food, like, may, table [3]\n",
      "entry, for, is, noise, topic [4,6,17,19,21]\n",
      "london, paris, people, please, six [5,7,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54]\n",
      "many, party, people, range, would [8]\n",
      "people, please, price, range, two [9]\n",
      "let, look, ok, option, think [8,10,12,16,18,20]\n",
      "actually, prefer, price, range, would [11,13]\n",
      "could, food, instead, people, silence [11,13,15]\n",
      "api_call, cheap, moderate, ok, option [14]\n",
      "actually, is, noise, prefer, would [15]\n",
      "anything, api_call, else, sure, update [12,14,16,18]\n",
      "actually, could, instead, silence, would [17]\n",
      "could, doe, instead, silence, work [19]\n",
      "api_call, else, moderate, sure, two [20]\n",
      "find, let, option, sure, think [22,24]\n",
      "address, great, like, look, silence [23]\n",
      "api_call, cheap, expensive, moderate, two [10,22,24]\n",
      "make, price, range, reservation, restaurant [3]\n",
      "cuisine, many, party, preference, type [6]\n",
      "cuisine, food, italian, love, spanish [7]\n",
      "bombay, madrid, please, rome, silence [9]\n",
      "doe, like, look, silence, work [21]\n",
      "doe, else, phone, something, work [23]\n"
     ]
    }
   ],
   "source": [
    "g = build_graph(gephi_csv_path, dialogs, max_st)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
