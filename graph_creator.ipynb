{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import json\n",
    "import random\n",
    "import artm\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from pyclustering.cluster.xmeans import xmeans\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "from pyclustering.cluster.optics import optics\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bert_serving.client import BertClient\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/predict/bigartm_batches’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/predict\n",
    "!mkdir data/predict/bigartm_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/bigartm_batches_babi’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "dialog_dataset_json = 'babi-1-6-full.json'\n",
    "vw_data_path = 'data/babi-st.vw'\n",
    "bigartm_batches_path = 'data/bigartm_batches_babi'\n",
    "max_st = 54\n",
    "num_topics = 2\n",
    "gephi_csv_path = 'data/babi-{}.csv'.format(max_st)\n",
    "!mkdir data/bigartm_batches_babi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/convai2_>3_batches_babi’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "dialog_dataset_json = 'convai2_>3.json'\n",
    "vw_data_path = 'data/convai2_>3-st.vw'\n",
    "bigartm_batches_path = 'data/convai2_>3_batches_babi'\n",
    "max_st = 25\n",
    "num_topics = 3\n",
    "gephi_csv_path = 'data/convai2_>3-{}.csv'.format(max_st)\n",
    "!mkdir 'data/convai2_>3_batches_babi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/convai1_batches_babi’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "dialog_dataset_json = 'f1-labeled-dialogs-with-dm.json'\n",
    "vw_data_path = 'data/convai1-st.vw'\n",
    "bigartm_batches_path = 'data/convai1_batches_babi'\n",
    "max_st = 30\n",
    "num_topics = 3\n",
    "gephi_csv_path = 'data/convai1-{}.csv'.format(max_st)\n",
    "!mkdir 'data/convai1_batches_babi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/multi-woz2_batches’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "dialog_dataset_json = 'multi-woz2.json'\n",
    "vw_data_path = 'data/multi-woz2-st.vw'\n",
    "bigartm_batches_path = 'data/multi-woz2_batches'\n",
    "max_st = 13\n",
    "num_topics = 7\n",
    "gephi_csv_path = 'data/multi-woz2-{}.csv'.format(max_st)\n",
    "!mkdir 'data/multi-woz2_batches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "def convert_to_vw(text, id_, user_id):\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokens = [t.lower() for t in tokenizer.tokenize(text)]\n",
    "    processed = []\n",
    "    for t in tokens:\n",
    "        l = lmtzr.lemmatize(t)\n",
    "        if l not in stopwords_en:\n",
    "            processed.append(l)\n",
    "    counted = Counter(processed)\n",
    "    res_str = str(id_)\n",
    "    for k, v in counted.items():\n",
    "        if v != 1:\n",
    "            res_str = res_str + \" {}:{}\".format(k, v)\n",
    "        else:\n",
    "            res_str = res_str + \" {}\".format(k)\n",
    "    res_str += ' {}'.format(user_id)\n",
    "    return res_str\n",
    "\n",
    "\n",
    "def convert_to_vw_data(sentences, vw_filename):\n",
    "    vw_file = open(vw_filename, 'w')\n",
    "    vw_data = []\n",
    "    ind = 0\n",
    "    for sent in sentences:\n",
    "        converted = convert_to_vw(sent, ind)\n",
    "        if len(converted.split(\" \")) > 1:\n",
    "            vw_data.append(convert_to_vw(sent, ind))\n",
    "            ind += 1\n",
    "    for row in vw_data:\n",
    "        print(row, file=vw_file)\n",
    "    vw_file.close()\n",
    "\n",
    "    \n",
    "def save_vw_to_file(sentences, vw_filename):\n",
    "    vw_file = open(vw_filename, 'w')\n",
    "    vw_data = []\n",
    "    for sent in sentences:\n",
    "        if len(sent.split(\" \")) > 1:\n",
    "            vw_data.append(sent)\n",
    "        else:\n",
    "            vw_data.append(\"this is noise entry for topic modelling\")\n",
    "    for row in vw_data:\n",
    "        print(row, file=vw_file)\n",
    "    vw_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_sentence_embedder():\n",
    "    # Other embedder https://tfhub.dev/google/universal-sentence-encoder-large/3\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "    embedder = hub.Module(module_url)\n",
    "    return embedder\n",
    "        \n",
    "def _embed_sentences(sentences):\n",
    "    embedder = _init_sentence_embedder()\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        embeddings = session.run(embedder(sentences))\n",
    "    return embeddings\n",
    "\n",
    "def _init_tfidf_vectorizer():\n",
    "    stop_words = []\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', norm='l2')\n",
    "    return vectorizer\n",
    "\n",
    "def _embed_sentences_with_tfidf(sentences):\n",
    "    vectorizer = _init_tfidf_vectorizer()\n",
    "    word_doc_matrix = vectorizer.fit_transform(sentences)\n",
    "    return {'vectorizer': vectorizer, 'word_doc_matrix': word_doc_matrix}\n",
    "\n",
    "def _embed_sentences_with_bert(sentences):\n",
    "    bc = BertClient()\n",
    "    return bc.encode(sentences)\n",
    "\n",
    "def _extract_df_data_naive(dialogs):\n",
    "    df_data = defaultdict(list)\n",
    "    id_ = 0\n",
    "    for dialog_id, dialog in dialogs.items():\n",
    "        current_ind_both = 1\n",
    "\n",
    "        thread = dialog['thread']\n",
    "\n",
    "        for row in thread:\n",
    "            text = row['text']\n",
    "            df_data['st'].append(current_ind_both)\n",
    "            df_data['sent'].append(text)\n",
    "            df_data['cluster_id'].append(None)\n",
    "            df_data['cluster_name'].append(None)\n",
    "            df_data['user_id'].append(row['userId'])\n",
    "            df_data['vw_sent'].append(convert_to_vw(text, id_, row['userId']))\n",
    "            df_data['topic_name'].append('')\n",
    "            df_data['topic_score'].append(0)\n",
    "            id_ += 1\n",
    "            current_ind_both += 1\n",
    "    return pd.DataFrame(data=df_data) \n",
    "\n",
    "\n",
    "def add_cluster_name_to_dialogs(json_filepath, df):\n",
    "    with open(json_filepath, 'r') as f:\n",
    "        dialogs = json.load(f)\n",
    "        \n",
    "    for dialog_id, dialog in dialogs.items():\n",
    "        thread = dialog['thread']\n",
    "\n",
    "        current_ind = 1\n",
    "\n",
    "        for row in thread:\n",
    "            text = row['text']\n",
    "            row['cluster_name'] = df[(df['st'] == current_ind) & (df['sent'] == text)].iloc[0]['topic_name_uniq_with_st']\n",
    "            current_ind += 1\n",
    "    return dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dialog_dataset_json, 'r') as f:\n",
    "    dialogs = json.load(f)\n",
    "dialogs = dict(list(dialogs.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = _extract_df_data_naive(dialogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    143048.000000\n",
       "mean          8.356943\n",
       "std           5.453840\n",
       "min           1.000000\n",
       "25%           4.000000\n",
       "50%           8.000000\n",
       "75%          12.000000\n",
       "max          44.000000\n",
       "Name: st, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.st.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0 0.0 55.68326187133789\n",
      "topic_0: User, hotel, stay, place, free, looking, need\n",
      "topic_1: User, information, called, taxi, book, looking, get\n",
      "topic_2: cambridge, User, train, need, looking, leaving, going\n",
      "topic_3: User, town, looking, place, go, centre, part\n",
      "topic_4: like, hi, would, User, hospital, good, know\n",
      "topic_5: find, help, User, please, want, price, priced\n",
      "topic_6: restaurant, User, looking, food, expensive, serf, cheap\n",
      "------\n",
      "2 0.0 0.0 100.6881103515625\n",
      "topic_0: Bot, price, range, hotel, area, guesthouse, preference\n",
      "topic_1: would, like, Bot, restaurant, area, sorry, food\n",
      "topic_2: Bot, sure, day, help, time, train, destination\n",
      "topic_3: Bot, else, museum, anything, cambridge, parkside, help\n",
      "topic_4: located, Bot, cambridge, number, phone, road, hospital\n",
      "topic_5: Bot, restaurant, town, centre, one, part, expensive\n",
      "topic_6: Bot, type, looking, area, particular, many, attraction\n",
      "------\n",
      "3 0.0 0.0 60.822017669677734\n",
      "topic_0: User, free, star, hotel, need, parking, wifi\n",
      "topic_1: like, would, User, food, thank, care, serf\n",
      "topic_2: User, cambridge, leave, need, arrive, leaving, 15\n",
      "topic_3: User, looking, town, centre, something, area, place\n",
      "topic_4: User, number, phone, address, postcode, please, get\n",
      "topic_5: User, yes, please, one, price, restaurant, range\n",
      "topic_6: User, book, people, please, table, night, yes\n",
      "------\n",
      "4 0.0 0.0 126.45431518554688\n",
      "topic_0: Bot, else, anything, help, need, number, today\n",
      "topic_1: would, like, Bot, book, house, sorry, criterion\n",
      "topic_2: Bot, train, time, cambridge, leaf, arrives, arrive\n",
      "topic_3: Bot, free, information, parking, college, museum, place\n",
      "topic_4: Bot, phone, number, located, address, postcode, road\n",
      "topic_5: Bot, restaurant, price, hotel, range, town, area\n",
      "topic_6: Bot, day, reference, reservation, table, number, many\n",
      "------\n",
      "5 0.0 0.0 61.33888244628906\n",
      "topic_0: User, one, thanks, hotel, free, star, need\n",
      "topic_1: User, would, like, thank, great, help, much\n",
      "topic_2: User, train, need, time, cambridge, arrive, leave\n",
      "topic_3: User, also, looking, place, find, town, centre\n",
      "topic_4: User, number, please, phone, address, get, could\n",
      "topic_5: User, price, food, range, restaurant, sure, try\n",
      "topic_6: User, yes, book, people, please, night, need\n",
      "------\n",
      "6 0.0 0.0 123.2259292602539\n",
      "topic_0: else, anything, help, Bot, number, today, need\n",
      "topic_1: would, like, Bot, book, reservation, time, sorry\n",
      "topic_2: Bot, train, day, time, cambridge, welcome, ticket\n",
      "topic_3: Bot, sure, many, information, free, need, entrance\n",
      "topic_4: Bot, phone, number, centre, street, road, address\n",
      "topic_5: Bot, restaurant, hotel, price, town, house, area\n",
      "topic_6: reference, number, Bot, wa, booking, successful, booked\n",
      "------\n",
      "7 0.0 0.0 61.035152435302734\n",
      "topic_0: User, need, thanks, hotel, free, stay, star\n",
      "topic_1: User, thank, like, would, help, great, much\n",
      "topic_2: User, train, cambridge, need, time, also, arrive\n",
      "topic_3: also, User, looking, place, town, yes, centre\n",
      "topic_4: User, please, number, one, get, address, phone\n",
      "topic_5: User, restaurant, price, range, expensive, food, area\n",
      "topic_6: User, yes, book, people, please, night, need\n",
      "------\n",
      "8 0.0 0.0 116.90625762939453\n",
      "topic_0: else, anything, Bot, help, need, number, today\n",
      "topic_1: would, like, Bot, book, stay, sorry, try\n",
      "topic_2: Bot, day, train, time, great, cambridge, welcome\n",
      "topic_3: Bot, centre, many, sure, phone, located, street\n",
      "topic_4: Bot, house, yes, free, road, parking, ha\n",
      "topic_5: Bot, hotel, restaurant, price, area, range, town\n",
      "topic_6: reference, number, Bot, wa, booking, successful, table\n",
      "------\n",
      "9 0.0 0.0 64.29500579833984\n",
      "topic_0: User, need, hotel, thanks, free, star, parking\n",
      "topic_1: User, thank, like, would, help, much, great\n",
      "topic_2: User, train, need, cambridge, 15, time, leave\n",
      "topic_3: User, also, looking, place, find, centre, town\n",
      "topic_4: User, number, night, get, phone, please, address\n",
      "topic_5: User, price, one, range, restaurant, food, expensive\n",
      "topic_6: please, User, yes, book, people, need, reference\n",
      "------\n",
      "10 0.0 0.0 118.76356506347656\n",
      "topic_0: else, anything, Bot, help, need, today, number\n",
      "topic_1: would, like, Bot, book, train, cambridge, leaf\n",
      "topic_2: Bot, day, time, thank, great, many, welcome\n",
      "topic_3: Bot, sure, centre, street, located, information, free\n",
      "topic_4: Bot, phone, number, house, yes, road, postcode\n",
      "topic_5: Bot, hotel, restaurant, price, area, star, range\n",
      "topic_6: number, reference, Bot, wa, booking, successful, table\n",
      "------\n",
      "11 0.0 0.0 62.29780197143555\n",
      "topic_0: User, thank, thanks, need, help, much, free\n",
      "topic_1: like, would, User, great, taxi, day, good\n",
      "topic_2: User, train, need, cambridge, time, 15, 30\n",
      "topic_3: User, also, looking, place, need, restaurant, town\n",
      "topic_4: User, people, night, address, phone, postcode, number\n",
      "topic_5: User, hotel, one, price, range, star, area\n",
      "topic_6: please, User, yes, number, book, reference, need\n",
      "------\n",
      "12 0.0 0.0 109.19017028808594\n",
      "topic_0: else, anything, help, Bot, number, today, booked\n",
      "topic_1: Bot, would, time, train, like, book, ticket\n",
      "topic_2: Bot, day, great, welcome, thank, enjoy, stay\n",
      "topic_3: Bot, many, sure, free, information, entrance, college\n",
      "topic_4: Bot, phone, located, number, address, postcode, road\n",
      "topic_5: Bot, would, like, hotel, restaurant, book, star\n",
      "topic_6: reference, number, Bot, wa, booking, successful, table\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "ldas_pickle_data = {}\n",
    "for i in range(1, max_st):    \n",
    "    sentences = df[df.st == i]['vw_sent']\n",
    "    save_vw_to_file(sentences, vw_data_path)\n",
    "    batch_vectorizer = artm.BatchVectorizer(data_path=vw_data_path, data_format='vowpal_wabbit',\n",
    "                                        target_folder='{}/{}'.format(bigartm_batches_path, i))\n",
    "    lda = artm.LDA(num_topics=num_topics, alpha=0.01, beta=0.001,\n",
    "               num_document_passes=5, dictionary=batch_vectorizer.dictionary)\n",
    "    lda.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=100)\n",
    "    print(i, lda.sparsity_phi_last_value, lda.sparsity_theta_last_value, lda.perplexity_value[-1])\n",
    "    top_tokens = lda.get_top_tokens(num_tokens=7)\n",
    "    topics = {}    \n",
    "    for j, token_list in enumerate(top_tokens):\n",
    "        topic_name = 'topic_' + str(j)\n",
    "        topic_value = \", \".join(token_list)\n",
    "        print('{}: {}'.format(topic_name, topic_value))\n",
    "        topics[topic_name] = topic_value\n",
    "    print(\"------\")\n",
    "    save_path = '{}/{}/dumped_model_{}'.format(bigartm_batches_path, i, i)\n",
    "    lda.save(save_path)\n",
    "    ldas_pickle_data[i] = {\n",
    "        'save_path': save_path,\n",
    "        'num_topics': num_topics,\n",
    "        'topics': topics\n",
    "    }\n",
    "    \n",
    "    theta = lda.transform(batch_vectorizer)\n",
    "    sentences_topics = []\n",
    "    sentences_topics_scores = [] \n",
    "    for k in range(min(len(sentences), theta.shape[1])):\n",
    "        topics_distribution = theta[k]\n",
    "        top1_topic = sorted(list(topics_distribution.items()), key=lambda x: x[1], reverse=True)[0]\n",
    "        topic_name = top1_topic[0]        \n",
    "        topic_str = \", \".join(sorted(set(topics[topic_name].split(\", \"))))        \n",
    "        sentences_topics.append(topic_str)\n",
    "        sentences_topics_scores.append(top1_topic[1])\n",
    "    \n",
    "    df.loc[df.st == i, 'topic_name'] = sentences_topics\n",
    "    df.loc[df.st == i, 'topic_score'] = sentences_topics_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_topic_names_mapping = {}\n",
    "topic_names = list(set(df['topic_name']))\n",
    "for i in range(len(topic_names)):\n",
    "    t1 = set(topic_names[i].split(\", \"))\n",
    "    t1_str = \", \".join(sorted(t1))\n",
    "    for j in range(i+1, len(topic_names)):\n",
    "        t2 = set(topic_names[j].split(\", \"))\n",
    "        if len(t1 - t2) <= 1:\n",
    "            t2_str = \", \".join(sorted(t2))\n",
    "            similar_topic_names_mapping[t1_str] = t2_str    \n",
    "    if not similar_topic_names_mapping.get(t1_str):\n",
    "        similar_topic_names_mapping[t1_str] = t1_str\n",
    "        \n",
    "ldas_pickle_data['sim_dict'] = similar_topic_names_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_name_uniq'] = df['topic_name'].map(lambda x: similar_topic_names_mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name_to_st = defaultdict(set)\n",
    "for _, row in df[['topic_name_uniq', 'st']].iterrows():\n",
    "    topic_name_to_st[row['topic_name_uniq']].add(row['st'])\n",
    "\n",
    "orig_topic_names_to_names_with_st = {}\n",
    "for topic_name in topic_name_to_st.keys():    \n",
    "    speech_turns = [str(e) for e in sorted(topic_name_to_st[topic_name])]\n",
    "    orig_topic_names_to_names_with_st[topic_name] = \"{} [{}]\".format(topic_name, \",\".join(speech_turns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_name_uniq_with_st'] = df['topic_name_uniq'].map(lambda x: orig_topic_names_to_names_with_st[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_sent_map = {}\n",
    "for _, row in df.iterrows():\n",
    "    st_sent_map[(row['st'], row['sent'])] = row['topic_name_uniq_with_st']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st_sent_map = {}\n",
    "# for _, row in df.iterrows():\n",
    "#     st_sent_map[(row['st'], row['sent'])] = \"{} [{}]\".format(row['topic_name_uniq'], row['st'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         User, free, hotel, looking, need, place, stay [1]\n",
       "1         Bot, area, attraction, looking, many, particul...\n",
       "2         User, free, hotel, need, parking, star, thanks...\n",
       "3         Bot, area, hotel, price, range, restaurant, st...\n",
       "4         User, book, need, night, people, please, yes [...\n",
       "5         Bot, book, like, reservation, sorry, time, wou...\n",
       "6         User, book, need, night, people, please, yes [...\n",
       "7         Bot, booking, number, reference, successful, t...\n",
       "8         User, great, help, like, much, thank, would [5...\n",
       "9          Bot, day, great, many, thank, time, welcome [10]\n",
       "10        User, book, called, get, information, looking,...\n",
       "11        Bot, anything, cambridge, else, help, museum, ...\n",
       "12        User, address, could, get, number, phone, plea...\n",
       "13        Bot, address, located, number, phone, postcode...\n",
       "14        User, address, could, get, number, phone, plea...\n",
       "15        Bot, address, centre, number, phone, road, str...\n",
       "16        User, great, help, like, much, thank, would [5...\n",
       "17        Bot, cambridge, day, ticket, time, train, welc...\n",
       "18        User, great, help, like, much, thank, would [5...\n",
       "19         Bot, day, great, many, thank, time, welcome [10]\n",
       "20        User, free, hotel, looking, need, place, stay [1]\n",
       "21        Bot, area, guesthouse, hotel, preference, pric...\n",
       "22        User, free, hotel, need, parking, star, thanks...\n",
       "23        Bot, book, criterion, house, like, sorry, woul...\n",
       "24        User, book, need, night, people, please, yes [...\n",
       "25        Bot, cambridge, day, ticket, time, train, welc...\n",
       "26        User, book, need, night, people, please, yes [...\n",
       "27        Bot, anything, booked, else, help, number, tod...\n",
       "28        User, arrive, cambridge, leave, need, time, tr...\n",
       "29         Bot, day, great, many, thank, time, welcome [10]\n",
       "                                ...                        \n",
       "143018    User, book, need, night, people, please, yes [...\n",
       "143019    Bot, booking, number, reference, successful, t...\n",
       "143020     User, free, help, much, need, thank, thanks [11]\n",
       "143021    Bot, day, enjoy, great, stay, thank, welcome [12]\n",
       "143022    User, cheap, expensive, food, looking, restaur...\n",
       "143023    Bot, centre, expensive, one, part, restaurant,...\n",
       "143024    User, book, need, night, people, please, yes [...\n",
       "143025    Bot, book, criterion, house, like, sorry, woul...\n",
       "143026    User, great, help, like, much, thank, would [5...\n",
       "143027    Bot, booking, number, reference, successful, t...\n",
       "143028    User, also, centre, looking, place, town, yes ...\n",
       "143029    Bot, area, hotel, price, range, restaurant, st...\n",
       "143030    User, also, centre, looking, place, town, yes ...\n",
       "143031    Bot, house, number, phone, postcode, road, yes...\n",
       "143032    User, book, need, people, please, reference, y...\n",
       "143033    Bot, address, located, number, phone, postcode...\n",
       "143034    User, cheap, expensive, food, looking, restaur...\n",
       "143035    User, cheap, expensive, food, looking, restaur...\n",
       "143036    User, cambridge, going, leaving, looking, need...\n",
       "143037    Bot, day, destination, help, sure, time, train...\n",
       "143038    15, User, arrive, cambridge, leave, leaving, n...\n",
       "143039    Bot, arrive, arrives, cambridge, leaf, time, t...\n",
       "143040    User, also, arrive, cambridge, need, time, tra...\n",
       "143041    Bot, cambridge, day, ticket, time, train, welc...\n",
       "143042    User, also, centre, looking, place, town, yes ...\n",
       "143043    Bot, centre, located, many, phone, street, sur...\n",
       "143044    User, address, could, get, number, phone, plea...\n",
       "143045    Bot, anything, booked, else, help, number, tod...\n",
       "143046     User, free, help, much, need, thank, thanks [11]\n",
       "143047    Bot, day, enjoy, great, stay, thank, welcome [12]\n",
       "Name: topic_name_uniq_with_st, Length: 143048, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['topic_name_uniq_with_st']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce69f5f136a9460fb7c946ef8caa0cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10438), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(dialog_dataset_json, 'r') as f:\n",
    "    dialogs = json.load(f)\n",
    "        \n",
    "    for dialog_id, dialog in tqdm_notebook(dialogs.items()):\n",
    "        thread = dialog['thread']\n",
    "\n",
    "        current_ind = 1\n",
    "\n",
    "        for row in thread:\n",
    "            text = row['text']\n",
    "            row['cluster_name'] = st_sent_map[(current_ind, text)]\n",
    "            current_ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(save_filename, dialogs, step_max=15):\n",
    "    graph_dict = defaultdict(int)\n",
    "    for dialog_id, dialog in dialogs.items():\n",
    "        thread = dialog['thread']\n",
    "\n",
    "        current_ind = 1\n",
    "        prev_row = None\n",
    "        row = None\n",
    "        for ind in range(len(thread)):\n",
    "            if prev_row:\n",
    "                row = thread[ind]\n",
    "\n",
    "            if not prev_row:\n",
    "                prev_row = thread[ind]\n",
    "                row = None\n",
    "\n",
    "            if row and current_ind < step_max:\n",
    "                edge = (prev_row['cluster_name'], row['cluster_name'])\n",
    "                graph_dict[edge] += 1\n",
    "                current_ind += 1\n",
    "                prev_row = row\n",
    "                row = None\n",
    "    G = nx.DiGraph()\n",
    "    weighted_edges = [(k[0], k[1], v) for k, v in graph_dict.items()]\n",
    "    G.add_weighted_edges_from(weighted_edges)\n",
    "\n",
    "    m = nx.adjacency_matrix(G).todense().astype(float)\n",
    "    m = np.squeeze(np.asarray(m))\n",
    "\n",
    "    for arr in m.tolist():\n",
    "        str_arr = [str(e) for e in arr]\n",
    "        print(\",\".join(str_arr))\n",
    "    print(\"\\n\".join(list(G.nodes)))\n",
    "    with open(save_filename, 'w') as f:\n",
    "        nodes = list(G.nodes)\n",
    "        print(\";\" + \";\".join(nodes), file=f)\n",
    "        for ind, arr in enumerate(m.tolist()):\n",
    "            str_arr = [nodes[ind]]\n",
    "            str_arr += [str(e) for e in arr]\n",
    "            print(\";\".join(str_arr), file=f)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0,156.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,67.0,0.0,0.0,0.0,0.0,992.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,53.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,135.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,49.0,0.0,0.0,0.0,158.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,177.0,0.0,97.0,0.0,0.0,0.0,0.0,0.0,0.0,433.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,54.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,323.0,0.0,0.0,0.0,0.0,443.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,173.0,0.0\n",
      "0.0,0.0,0.0,629.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,55.0,0.0,0.0,0.0,379.0,38.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,43.0,0.0,0.0,0.0,0.0,198.0,0.0,0.0,0.0,0.0,0.0,24.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,833.0,0.0,0.0,298.0,0.0,0.0,0.0,616.0,0.0,0.0,0.0,0.0,0.0,0.0,36.0,83.0,0.0,0.0,0.0,0.0,0.0,0.0,868.0,0.0,222.0,0.0,0.0,0.0,0.0,143.0,0.0,0.0,0.0,40.0,579.0,0.0,0.0,432.0,435.0,0.0,406.0,0.0,0.0,0.0,183.0,0.0,0.0,0.0,435.0,0.0,0.0,75.0,0.0,0.0\n",
      "0.0,0.0,0.0,199.0,0.0,396.0,2361.0,0.0,186.0,0.0,0.0,0.0,20.0,38.0,284.0,0.0,174.0,1069.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,532.0,0.0,37.0,0.0,0.0,7.0,98.0,54.0,0.0,0.0,0.0,22.0,0.0,0.0,0.0,0.0,0.0,0.0,298.0,0.0,0.0,0.0,29.0,0.0,0.0,0.0,182.0,0.0,124.0,76.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,443.0,0.0,0.0,59.0,0.0,0.0,0.0,143.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,105.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,172.0,0.0,0.0,0.0,63.0,201.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,100.0,0.0,0.0,533.0,0.0,0.0,0.0,88.0,0.0,0.0,0.0,0.0,0.0,0.0,161.0,73.0,0.0,762.0,0.0,0.0,0.0,0.0,329.0,0.0,52.0,0.0,0.0,0.0,0.0,152.0,0.0,0.0,0.0,186.0,40.0,0.0,0.0,680.0,147.0,0.0,12.0,0.0,0.0,0.0,394.0,0.0,0.0,0.0,0.0,0.0,0.0,53.0,0.0,0.0\n",
      "0.0,0.0,0.0,189.0,0.0,73.0,81.0,0.0,754.0,0.0,0.0,0.0,0.0,174.0,1430.0,0.0,0.0,780.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,97.0,0.0,0.0,0.0,92.0,207.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,94.0,0.0,0.0,0.0,64.0,0.0,0.0,0.0,80.0,0.0,145.0,103.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,426.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,75.0,0.0,0.0,0.0,0.0,38.0,0.0,0.0,0.0,0.0,48.0,0.0,0.0,0.0,0.0,0.0,123.0,0.0,0.0,0.0,105.0,0.0,0.0,0.0,0.0,0.0,0.0,33.0,0.0,0.0\n",
      "0.0,121.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,173.0,0.0,0.0,0.0,0.0,142.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,376.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,123.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,210.0,0.0,0.0,0.0,230.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,25.0,0.0,42.0,0.0,0.0,0.0,0.0,0.0,0.0,451.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,18.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,121.0,0.0,0.0,0.0,0.0,130.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,33.0,0.0\n",
      "0.0,0.0,0.0,219.0,0.0,63.0,162.0,0.0,47.0,0.0,0.0,0.0,825.0,634.0,144.0,0.0,57.0,1025.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,62.0,0.0,345.0,0.0,0.0,441.0,62.0,316.0,0.0,0.0,0.0,19.0,0.0,0.0,0.0,0.0,0.0,0.0,91.0,0.0,0.0,0.0,233.0,0.0,0.0,0.0,70.0,0.0,264.0,353.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,134.0,0.0,0.0,428.0,0.0,0.0,0.0,156.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,543.0,0.0,0.0,0.0,0.0,125.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,118.0,0.0,0.0,0.0,182.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,57.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,134.0,0.0,0.0,346.0,0.0,0.0,0.0,146.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,133.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,132.0,0.0,0.0,0.0,129.0,64.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,636.0,0.0,0.0,380.0,0.0,0.0,0.0,196.0,0.0,0.0,0.0,0.0,0.0,0.0,767.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,239.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,674.0,0.0,0.0,0.0,405.0,130.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,710.0,0.0,150.0,0.0,0.0,0.0,0.0,0.0,0.0,104.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,16.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,209.0,0.0,0.0,0.0,0.0,270.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,312.0,0.0\n",
      "0.0,0.0,0.0,0.0,429.0,0.0,0.0,86.0,0.0,0.0,0.0,137.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,252.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,144.0,0.0,0.0,0.0,110.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,248.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,132.0,0.0,0.0,955.0,0.0,0.0,0.0,195.0,0.0,0.0,0.0,0.0,0.0,0.0,121.0,77.0,0.0,823.0,0.0,0.0,0.0,0.0,362.0,0.0,57.0,0.0,0.0,0.0,0.0,114.0,0.0,0.0,0.0,315.0,46.0,0.0,0.0,566.0,126.0,0.0,27.0,0.0,0.0,0.0,409.0,0.0,0.0,0.0,64.0,0.0,0.0,57.0,0.0,0.0\n",
      "0.0,0.0,0.0,16.0,0.0,0.0,95.0,0.0,400.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,62.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,14.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,10.0,0.0,0.0,0.0,781.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,113.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0,73.0,0.0,0.0,715.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,47.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,81.0,0.0,0.0,0.0,37.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1084.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,340.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,43.0,0.0,0.0,0.0,0.0,245.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,43.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,554.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,43.0,0.0,0.0,0.0,617.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,86.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,42.0,0.0,0.0,0.0,0.0,41.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1954.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,96.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,14.0,0.0,0.0,0.0,11.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,89.0,0.0,70.0,0.0,0.0,0.0,0.0,0.0,0.0,97.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2003.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,123.0,0.0,0.0,0.0,0.0,120.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,68.0,0.0\n",
      "0.0,0.0,0.0,17.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6.0,0.0,0.0,0.0,153.0,157.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,199.0,0.0,0.0,0.0,0.0,4.0,0.0,0.0,0.0,0.0,0.0,1657.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,291.0,0.0,0.0,164.0,0.0,0.0,0.0,75.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,69.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,262.0,0.0,0.0,0.0,160.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,45.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,988.0,0.0,222.0,49.0,0.0,150.0,0.0,0.0,0.0,0.0,87.0,261.0,0.0,0.0,202.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,92.0,0.0,0.0,0.0,666.0,259.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,187.0,0.0,0.0,0.0,27.0,0.0,0.0,0.0,88.0,0.0,118.0,28.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,47.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,87.0,0.0,0.0,0.0,0.0,83.0,0.0,0.0,0.0,0.0,43.0,0.0,0.0,0.0,0.0,0.0,94.0,0.0,0.0,0.0,249.0,0.0,0.0,0.0,0.0,0.0,0.0,59.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,342.0,0.0,0.0,0.0,0.0,0.0,114.0,0.0,0.0,0.0,0.0,211.0,0.0,0.0,125.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,72.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,54.0,0.0,0.0,0.0,203.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1497.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,130.0,0.0,0.0,168.0,0.0,0.0,0.0,396.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,168.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,135.0,0.0,0.0,0.0,264.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,56.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,550.0,0.0,0.0,187.0,0.0,0.0,0.0,454.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,272.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,44.0,0.0,0.0,0.0,130.0,338.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,162.0,0.0,0.0,204.0,0.0,0.0,0.0,153.0,0.0,0.0,0.0,0.0,0.0,0.0,70.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,106.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,94.0,68.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,10.0,0.0,0.0,0.0,0.0,0.0,77.0,0.0,0.0,0.0,0.0,64.0,0.0,0.0,55.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,331.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,17.0,0.0,0.0,0.0,239.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,135.0,0.0,101.0,0.0,0.0,0.0,0.0,0.0,0.0,126.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,87.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,351.0,0.0,0.0,0.0,0.0,313.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,163.0,0.0\n",
      "0.0,0.0,0.0,625.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,99.0,0.0,0.0,0.0,223.0,228.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,139.0,0.0,0.0,0.0,0.0,122.0,0.0,0.0,0.0,0.0,0.0,61.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,631.0,0.0,0.0,110.0,0.0,0.0,0.0,112.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,62.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,836.0,0.0,0.0,0.0,25.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,49.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,14.0,0.0,141.0,144.0,0.0,0.0,0.0,0.0,0.0,0.0,23.0,2506.0,0.0,0.0,156.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,14.0,11.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,115.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,50.0,25.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,48.0,0.0,0.0,0.0,0.0,0.0,93.0,0.0,0.0,0.0,0.0,47.0,0.0,0.0,34.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,668.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,22.0,0.0,0.0,0.0,105.0,0.0,0.0,0.0,0.0\n",
      "0.0,801.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,407.0,0.0,0.0,0.0,0.0,201.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,48.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,235.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,124.0,0.0,0.0,0.0,364.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,539.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,118.0,0.0,0.0,0.0,244.0,74.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,67.0,0.0,0.0,0.0,0.0,450.0,0.0,0.0,0.0,0.0,0.0,26.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,861.0,0.0,160.0,33.0,0.0,38.0,0.0,0.0,0.0,0.0,128.0,65.0,0.0,0.0,120.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,80.0,0.0,0.0,0.0,582.0,82.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,198.0,0.0,0.0,0.0,522.0,0.0,0.0,0.0,83.0,0.0,236.0,472.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,1001.0,0.0,0.0,111.0,0.0,27.0,0.0,0.0,0.0,0.0,0.0,30.0,0.0,0.0,98.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,72.0,0.0,0.0,0.0,0.0,81.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,170.0,0.0,0.0,0.0,99.0,0.0,0.0,0.0,131.0,0.0,0.0,76.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,281.0,0.0,0.0,71.0,0.0,0.0,0.0,134.0,0.0,0.0,0.0,0.0,0.0,0.0,145.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,149.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,104.0,242.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,236.0,0.0,0.0,0.0,0.0,0.0,214.0,0.0,0.0,0.0,0.0,200.0,0.0,0.0,42.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,151.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,29.0,0.0,0.0,0.0,102.0,0.0,0.0,0.0,0.0\n",
      "0.0,76.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,37.0,0.0,0.0,0.0,0.0,43.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,31.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,38.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,218.0,0.0,0.0,0.0,42.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,58.0,0.0,124.0,0.0,0.0,0.0,0.0,0.0,0.0,339.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,12.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,116.0,0.0,0.0,0.0,0.0,53.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,39.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,42.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,204.0,0.0,0.0,0.0,0.0,190.0,0.0,0.0,0.0,0.0,115.0,0.0,0.0,0.0,0.0,0.0,159.0,0.0,0.0,0.0,195.0,0.0,0.0,0.0,0.0,0.0,0.0,63.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,14.0,0.0,0.0,0.0,0.0,0.0,35.0,0.0,0.0,0.0,0.0,157.0,0.0,0.0,59.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,179.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1255.0,0.0,0.0,0.0,47.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,361.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,172.0,0.0,386.0,0.0,0.0,0.0,0.0,0.0,0.0,348.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,254.0,0.0,0.0,0.0,0.0,189.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,204.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,326.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,424.0,0.0,0.0,0.0,0.0,73.0,0.0,0.0,0.0,0.0,146.0,0.0,0.0,0.0,0.0,0.0,153.0,0.0,0.0,0.0,211.0,0.0,0.0,0.0,0.0,0.0,0.0,50.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,150.0,22.0,0.0,0.0,0.0,0.0,0.0,0.0,122.0,50.0,0.0,0.0,48.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,494.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,68.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,186.0,0.0,0.0,163.0,0.0,0.0,0.0,189.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,761.0,0.0,0.0,0.0,0.0,104.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,141.0,0.0,0.0,0.0,129.0,70.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,144.0,0.0,0.0,211.0,0.0,0.0,0.0,344.0,0.0,0.0,0.0,0.0,0.0,0.0,78.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,81.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,187.0,75.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,14.0,0.0,0.0,0.0,0.0,0.0,24.0,0.0,0.0,0.0,0.0,89.0,0.0,0.0,71.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,73.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,74.0,0.0,0.0,0.0,45.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,457.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,101.0,0.0,0.0,0.0,183.0,43.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,68.0,0.0,0.0,0.0,0.0,116.0,0.0,0.0,0.0,0.0,0.0,24.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,122.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,52.0,0.0,0.0,0.0,0.0,107.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,65.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,95.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,84.0,0.0,0.0,0.0,134.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "User, free, hotel, looking, need, place, stay [1]\n",
      "Bot, area, attraction, looking, many, particular, type [2]\n",
      "User, free, hotel, need, parking, star, thanks [3]\n",
      "Bot, area, hotel, price, range, restaurant, star [4,8,10]\n",
      "User, book, need, night, people, please, yes [3,5,7,9]\n",
      "Bot, book, like, reservation, sorry, time, would [6]\n",
      "Bot, booking, number, reference, successful, table, wa [6,8,10,12]\n",
      "User, great, help, like, much, thank, would [5,7,9]\n",
      "Bot, day, great, many, thank, time, welcome [10]\n",
      "User, book, called, get, information, looking, taxi [1]\n",
      "Bot, anything, cambridge, else, help, museum, parkside [2]\n",
      "User, address, could, get, number, phone, please [3,5,7,9]\n",
      "Bot, address, located, number, phone, postcode, road [4,12]\n",
      "Bot, address, centre, number, phone, road, street [6]\n",
      "Bot, cambridge, day, ticket, time, train, welcome [6,8]\n",
      "Bot, area, guesthouse, hotel, preference, price, range [2]\n",
      "Bot, book, criterion, house, like, sorry, would [4]\n",
      "Bot, anything, booked, else, help, number, today [4,6,8,10,12]\n",
      "User, arrive, cambridge, leave, need, time, train [9]\n",
      "15, 30, User, cambridge, need, time, train [11]\n",
      "Bot, book, like, ticket, time, train, would [12]\n",
      "User, cheap, expensive, food, looking, restaurant, serf [1,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44]\n",
      "User, cambridge, going, leaving, looking, need, train [1]\n",
      "Bot, day, destination, help, sure, time, train [2]\n",
      "15, User, arrive, cambridge, leave, leaving, need [3]\n",
      "Bot, day, many, number, reference, reservation, table [4]\n",
      "User, free, hotel, need, star, stay, thanks [5,7,9]\n",
      "Bot, house, number, phone, postcode, road, yes [10]\n",
      "User, book, need, people, please, reference, yes [11]\n",
      "Bot, book, hotel, like, restaurant, star, would [12]\n",
      "Bot, college, free, information, museum, parking, place [4]\n",
      "Bot, area, hotel, house, price, restaurant, town [6]\n",
      "Bot, free, ha, house, parking, road, yes [8]\n",
      "User, also, looking, need, place, restaurant, town [11]\n",
      "Bot, area, food, like, restaurant, sorry, would [2]\n",
      "User, care, food, like, serf, thank, would [3]\n",
      "Bot, arrive, arrives, cambridge, leaf, time, train [4]\n",
      "User, also, arrive, cambridge, need, time, train [5,7]\n",
      "User, area, hotel, one, price, range, star [11]\n",
      "User, centre, go, looking, part, place, town [1]\n",
      "User, area, centre, looking, place, something, town [3]\n",
      "User, also, centre, looking, place, town, yes [5,7,9]\n",
      "User, expensive, food, one, price, range, restaurant [7,9]\n",
      "Bot, book, like, sorry, stay, try, would [8]\n",
      "User, address, night, number, people, phone, postcode [11]\n",
      "User, good, hi, hospital, know, like, would [1]\n",
      "Bot, cambridge, hospital, located, number, phone, road [2]\n",
      "Bot, centre, free, information, located, street, sure [10]\n",
      "User, free, help, much, need, thank, thanks [11]\n",
      "Bot, day, enjoy, great, stay, thank, welcome [12]\n",
      "Bot, centre, expensive, one, part, restaurant, town [2]\n",
      "Bot, book, cambridge, leaf, like, train, would [10]\n",
      "User, food, price, range, restaurant, sure, try [5]\n",
      "Bot, entrance, free, information, many, need, sure [6,12]\n",
      "Bot, centre, located, many, phone, street, sure [8]\n",
      "User, day, good, great, like, taxi, would [11]\n",
      "User, one, please, price, range, restaurant, yes [3]\n",
      "User, find, help, please, price, priced, want [1]\n"
     ]
    }
   ],
   "source": [
    "g = build_graph(gephi_csv_path, dialogs, max_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using existing topic modelling\n",
    "\n",
    "sentence => topic => unified topic name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/babi_lda_data.pickle', 'wb') as f:\n",
    "    pickle.dump(ldas_pickle_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topic(sentence, lda, topics, user_id):\n",
    "    id_ = 1\n",
    "    vw_pred_sents = [convert_to_vw(sentence, id_, user_id)]\n",
    "    vw_pred_data_path = 'data/predict/pred.txt'\n",
    "    save_vw_to_file(vw_pred_sents, vw_pred_data_path)\n",
    "    bigartm_pred_batches_path = 'data/predict/bigartm_batches'\n",
    "    batch_vectorizer = artm.BatchVectorizer(data_path=vw_pred_data_path, data_format='vowpal_wabbit',\n",
    "                                    target_folder=bigartm_pred_batches_path)\n",
    "    try:\n",
    "        theta = lda.transform(batch_vectorizer)\n",
    "    except artm.wrapper.exceptions.InvalidOperationException:\n",
    "        return 'UNK', 1\n",
    "\n",
    "    sentences_topics = []\n",
    "    sentences_topics_scores = [] \n",
    "    for k in range(min(len(vw_pred_sents), theta.shape[1])):\n",
    "        topics_distribution = theta[k]\n",
    "        top1_topic = sorted(list(topics_distribution.items()), key=lambda x: x[1], reverse=True)[0]\n",
    "        topic_name = top1_topic[0]        \n",
    "        topic_str = \", \".join(sorted(set(topics[topic_name].split(\", \"))))        \n",
    "        sentences_topics.append(topic_str)\n",
    "        sentences_topics_scores.append(top1_topic[1])\n",
    "    return sentences_topics[0], sentences_topics_scores[0]\n",
    "\n",
    "ldas_cache = {}\n",
    "def load_lda(ldas_pickle_data, ind):\n",
    "    if ind in ldas_cache:\n",
    "        return ldas_cache[ind]\n",
    "    new_lda = artm.LDA(num_topics=ldas_pickle_data[ind]['num_topics'])\n",
    "    new_lda.load(ldas_pickle_data[ind]['save_path'])\n",
    "    ldas_cache[ind] = new_lda\n",
    "    return new_lda\n",
    "\n",
    "def predict_with_label_replace(sentence, lda, topics, user_id):\n",
    "    topic, score = predict_topic(sentence, lda, topics, user_id)\n",
    "    if topic != 'UNK':\n",
    "        if topic not in similar_topic_names_mapping:\n",
    "            print(\"Warning {}\".format(topic))\n",
    "        else:\n",
    "            topic = similar_topic_names_mapping[topic]\n",
    "    topic = topic.split(\", \")\n",
    "    return \"_\".join(topic)\n",
    "\n",
    "def parse_dialogs(filename, with_history, ignore_options):\n",
    "    dialogs = []\n",
    "    with open(filename, 'r') as f:\n",
    "        dialog = []\n",
    "        st = 1\n",
    "        for line in tqdm_notebook(f):\n",
    "            if line.strip() == '':\n",
    "                st = 1\n",
    "                dialogs.append(dialog)\n",
    "                dialog = []\n",
    "            else:\n",
    "                splitted = line.strip().split('\\t')\n",
    "                if len(splitted) == 1 and ignore_options:\n",
    "                    continue\n",
    "                elif len(splitted) == 1:\n",
    "                    raise ValueError('Line has not 2 utterances (seems like an option) {}'.format(splitted))\n",
    "                user_utt, bot_utt = splitted\n",
    "                utt_num = user_utt.split(' ')[0]\n",
    "                user_utt = ' '.join(user_utt.split(' ')[1:])\n",
    "                if user_utt == '':\n",
    "                    user_utt = '<SILENCE>'\n",
    "\n",
    "                if bot_utt == '':\n",
    "                    bot_utt = '<SILENCE>'\n",
    "                new_lda_user = load_lda(ldas_pickle_data, st)\n",
    "                new_lda_bot = load_lda(ldas_pickle_data, st+1)\n",
    "                user_utt = predict_with_label_replace(user_utt, new_lda_user, ldas_pickle_data[st]['topics'], 'User')\n",
    "                bot_utt = predict_with_label_replace(bot_utt, new_lda_bot, ldas_pickle_data[st+1]['topics'], 'Bot')\n",
    "                \n",
    "                if with_history:\n",
    "                    if len(dialog) > 0:\n",
    "                        prev_step = dialog[len(dialog) - 1]\n",
    "                        user_utt_with_history = \"{} {} {}\".format(prev_step[1], prev_step[2], user_utt)\n",
    "                    else:\n",
    "                        user_utt_with_history = user_utt\n",
    "                    dialog.append((utt_num, user_utt_with_history, bot_utt))\n",
    "                else:\n",
    "                    dialog.append((utt_num, user_utt, bot_utt))\n",
    "                st += 1\n",
    "    return dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2be067bfa946e78b3734699550cbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n",
      "Warning User, silence\n"
     ]
    }
   ],
   "source": [
    "with_history, ignore_options = True, True\n",
    "dialogs = parse_dialogs('../supervised-embedding-model/data/dialog-bAbI-tasks/dialog-babi-task5-full-dialogs-tst.txt', with_history, ignore_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../supervised-embedding-model/data/test-task-5-topic.tsv', 'w') as f:\n",
    "    for dialog in dialogs:\n",
    "        for _, user_utt, bot_utt in dialog:\n",
    "            print('{}\\t{}'.format(user_utt, bot_utt), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../supervised-embedding-model/data/test-task-5-topic.tsv ../supervised-embedding-model/data/train-task-5-topic.tsv ../supervised-embedding-model/data/dev-task-5-topic.tsv | python ../supervised-embedding-model/build_vocabulary.py > ../supervised-embedding-model/data/vocab-task-5-topic.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tBot_find_let_look_option_sure_think\r\n",
      "1\tUser_people_please_price_range_restaurant_two\r\n",
      "2\tUser_great_let_like_look_perfect_silence\r\n",
      "3\tUNK\r\n",
      "4\tBot_else_let_look_ok_option_update\r\n",
      "5\tUser_cuisine_food_french_italian_love_spanish\r\n",
      "6\tUser_doe_number_phone_something_thanks_work\r\n",
      "7\tUser_silence\r\n",
      "8\tUser_great_like_look_rock_silence_thank\r\n",
      "9\tUser_address_like_provide_rock_silence_thank\r\n",
      "10\tBot_anything_api_call_cheap_else_sure_update\r\n",
      "11\tBot_find_let_look_ok_option_think\r\n",
      "12\tBot_api_call_cheap_expensive_four_moderate_six\r\n",
      "13\tBot_api_call_else_moderate_sure_two_update\r\n",
      "14\tUser_could_cuisine_food_instead_silence_six\r\n",
      "15\tBot_anything_else_let_look_sure_update\r\n",
      "16\tUser_actually_moderate_prefer_price_range_would\r\n",
      "17\tUser_address_doe_else_perfect_something_work\r\n",
      "18\tUser_could_four_instead_people_silence_six\r\n",
      "19\tUser_actually_could_instead_prefer_work_would\r\n",
      "20\tBot_hello_help_today\r\n",
      "21\tBot_anything_great_let_option_reservation_think\r\n",
      "22\tBot_api_call_cheap_let_look_moderate_ok\r\n",
      "23\tUser_london_madrid_paris_people_please_six\r\n",
      "24\tBot_cuisine_looking_preference_price_range_type\r\n",
      "25\tUser_good_hello_hi_morning\r\n",
      "26\tUser_doe_else_phone_restaurant_something_work\r\n",
      "27\tBot_let_look_ok_option_price_range\r\n",
      "28\tBot_many_party_people_price_range_would\r\n",
      "29\tBot_api_call_find_let_moderate_option_sure\r\n",
      "30\tUser_book_food_like_may_people_table\r\n",
      "31\tUser_bombay_london_madrid_please_rome_silence\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../supervised-embedding-model/data/vocab-task-5-topic.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'User_good_hello_hi_morning'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hi! I'd like to select italian cousine!\"\n",
    "\n",
    "ind = 1\n",
    "new_lda = load_lda(ldas_pickle_data, ind)\n",
    "predict_with_label_replace(sentence, new_lda, ldas_pickle_data[ind]['topics'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
